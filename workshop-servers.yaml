version: '3.4'

services:
  kafka:
    image: ${KAFKA_IMAGE}
    labels:
        component: kafka
    entrypoint:
        - /wait-for-it.sh
        - zookeeper:2181
        - --timeout=300
        - --
        - /custom_entrypoint.sh
    command:
      - /etc/confluent/docker/run
    networks:
      - workshop
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${KAFKA_HOST}:9092
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_DELETE_TOPICS: "true"
      KAFKA_AUTO_CREATE_TOPICS: "false"
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_TRANSACTION_MAX_TIMEOUT_MS: 3600000
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOGS_PATH: /data/kafka-logs
      KAFKA_HEAP_OPTS: "-Xmx800M -Xms800M"
    volumes:
      - kafka-data:/data
    logging:
      options:
        labels: "component"
    ports:
      - target: 9092
        published: 9092
        protocol: tcp
        mode: ingress
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        failure_action: rollback
        delay: 30s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 120s
      resources:
        limits:
          cpus: '0.5'
          memory: 900M
        reservations:
          cpus: '0.2'
          memory: 900M

  zookeeper:
    image: ${ZOOKEEPER_IMAGE}
    labels:
        component: zookeeper
    networks:
      - workshop
    environment:
      ZOO_MY_ID: 1
    volumes:
      - zookeeper-data:/data
      - zookeeper-datalog:/datalog
    logging:
      options:
        labels: "component"
    ports:
      - target: 2181
        published: 2181
        protocol: tcp
        mode: ingress
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        failure_action: rollback
        delay: 30s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 120s
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 256M

  cadvisor:
    image: google/cadvisor
    labels:
        component: cadvisor
    networks:
      - workshop
    command: -logtostderr -docker_only
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /:/rootfs:ro
      - /var/run:/var/run
      - /sys:/sys:ro
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
      - /var/lib/docker/:/var/lib/docker:ro
    logging:
      options:
        labels: "component"
    deploy:
      mode: global
      update_config:
        parallelism: 1
        failure_action: rollback
        delay: 30s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 120s
      resources:
        limits:
          cpus: '0.5'
          memory: 32M
        reservations:
          cpus: '0.01'
          memory: 16M

  grafana:
    image: ${GRAFANA_IMAGE}
    labels:
        component: grafana
    networks:
      - workshop
    environment:
      - GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      #- GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL:-localhost}
      #- GF_SMTP_ENABLED=${GF_SMTP_ENABLED:-false}
      #- GF_SMTP_FROM_ADDRESS=${GF_SMTP_FROM_ADDRESS:-grafana@test.com}
      #- GF_SMTP_FROM_NAME=${GF_SMTP_FROM_NAME:-Grafana}
      #- GF_SMTP_HOST=${GF_SMTP_HOST:-smtp:25}
      #- GF_SMTP_USER=${GF_SMTP_USER}
      #- GF_SMTP_PASSWORD=${GF_SMTP_PASSWORD}
    ports:
      - target: 3000
        published: 8081
        protocol: tcp
        mode: ingress
    volumes:
      - grafana:/var/lib/grafana
    logging:
      options:
        labels: "component"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        failure_action: rollback
        delay: 30s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 120s
      resources:
        limits:
          cpus: '0.5'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M

  alertmanager:
    image: ${ALERTMANAGER_IMAGE}
    labels:
        component: alertmanager
    networks:
      - workshop
    environment:
      - SLACK_URL=${SLACK_URL:-https://hooks.slack.com/services/TOKEN}
      - SLACK_CHANNEL=${SLACK_CHANNEL:-general}
      - SLACK_USER=${SLACK_USER:-alertmanager}
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    ports:
      - target: 9093
        published: 9093
        protocol: tcp
        mode: ingress
    volumes:
      - alertmanager:/alertmanager
    logging:
      options:
        labels: "component"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        failure_action: rollback
        delay: 30s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 120s
      resources:
        limits:
          cpus: '0.5'
          memory: 32M
        reservations:
          cpus: '0.01'
          memory: 16M

  unsee:
    image: cloudflare/unsee:v0.9.2
    labels:
        component: unsee
    networks:
      - workshop
    environment:
      - "ALERTMANAGER_URIS=default:http://alertmanager:9093"
    logging:
      options:
        labels: "component"
    deploy:
      mode: replicated
      replicas: 1
      update_config:
        parallelism: 1
        failure_action: rollback
        delay: 30s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 120s
      resources:
        limits:
          cpus: '0.5'
          memory: 32M
        reservations:
          cpus: '0.01'
          memory: 16M

  node-exporter:
    image: ${NODEEXPORTER_IMAGE}
    labels:
        component: node-exporter
    networks:
      - workshop
    environment:
      - NODE_ID={{.Node.ID}}
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /etc/hostname:/etc/nodename
    command:
      - '--path.sysfs=/host/sys'
      - '--path.procfs=/host/proc'
      - '--collector.textfile.directory=/etc/node-exporter/'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
      # no collectors are explicitely enabled here, because the defaults are just fine,
      # see https://github.com/prometheus/node_exporter
      # disable ipvs collector because it barfs the node-exporter logs full with errors on my centos 7 vm's
      - '--no-collector.ipvs'
    logging:
      options:
        labels: "component"
    deploy:
      mode: global
      update_config:
        parallelism: 1
        failure_action: rollback
        delay: 30s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 120s
      resources:
        limits:
          cpus: '0.5'
          memory: 32M
        reservations:
          cpus: '0.01'
          memory: 16M

  prometheus:
    image: ${PROMETHEUS_IMAGE}
    labels:
        component: prometheus
    environment:
      - ENVIRONMENT=${ENVIRONMENT}
    networks:
      - workshop
    volumes:
      - prometheus:/prometheus
    ports:
      - target: 9090
        published: 9090
        protocol: tcp
        mode: ingress
    logging:
      options:
        labels: "component"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        failure_action: rollback
        delay: 30s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 120s
      resources:
        limits:
          cpus: '0.5'
          memory: 1024M
        reservations:
          cpus: '0.1'
          memory: 256M

  dockerd-exporter:
    image: stefanprodan/caddy
    labels:
        component: dockerd-exporter
    networks:
      - workshop
    environment:
      - DOCKER_GWBRIDGE_IP=172.18.0.1
    configs:
      - source: dockerd_config
        target: /etc/caddy/Caddyfile
    logging:
      options:
        labels: "component"
    deploy:
      mode: global
      resources:
        limits:
          cpus: '0.5'
          memory: 32M
        reservations:
          cpus: '0.01'
          memory: 16M

  # caddy:
  #   image: stefanprodan/caddy
  #   labels:
  #       component: caddy
  #   ports:
  #     - "3000:3000"
  #     - "9090:9090"
  #     - "9093:9093"
  #     - "9094:9094"
  #   networks:
  #     - workshop
  #   environment:
  #     - ADMIN_USER=${ADMIN_USER:-admin}
  #     - ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
  #   configs:
  #     - source: caddy_config
  #       target: /etc/caddy/Caddyfile
  #   logging:
  #     options:
  #       labels: "component"
  #   deploy:
  #     mode: replicated
  #     replicas: 1
  #     placement:
  #       constraints:
  #         - node.role == manager
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: 128M
  #       reservations:
  #         cpus: '0.1'
  #         memory: 64M
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:3000"]
  #     interval: 5s
  #     timeout: 1s
  #     retries: 5

  graphite:
    labels:
        component: graphite
    image: graphiteapp/graphite-statsd
    ports:
      - target: 80
        published: 8080
        protocol: tcp
        mode: ingress
      - target: 2003
        published: 2003
        protocol: tcp
        mode: ingress
    networks:
      - workshop
    volumes:
      - graphite:/opt/graphite/storage
    logging:
      options:
        labels: "component"
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        failure_action: rollback
        delay: 30s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 120s
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M

networks:
  workshop:
    external: true

volumes:
    kafka-data:
    zookeeper-data:
    zookeeper-datalog:
    prometheus:
    grafana:
    alertmanager:
    graphite:

configs:
  caddy_config:
    file: ./caddy/Caddyfile
  dockerd_config:
    file: ./dockerd-exporter/Caddyfile
